Linux TCP UDP 网络性能优化
===============

内核缓冲区
----------

查看ubuntu 18.04系统默认的套接字缓冲区大小：

    $ sudo sysctl net.core.wmem_default
    net.core.wmem_default = 212992
    $ sysctl net.core.rmem_default
    net.core.rmem_default = 212992
    $ sysctl net.core.rmem_max
    sysctl net.core.rmem_max
    $ sysctl net.core.wmem_max
    net.core.wmem_max = 212992

212992 bytes，换算成 KB，212992 / 1024 = 208 KB

这些参数显示分配给任何类型连接的默认和最大写入、读取缓冲区大小。 由于分配的空间来自RAM，因此默认值设置总是有点低。增加这一点可能会提高运行NFS等服务器的系统的性能。 将它们增加到256k / 4MB将最有效，否则您必须对这些值进行基准测试，以找到系统配置的理想值

默认是 208 KB，_default 值可以不改，_max 值修改一下。我们把自定义网络优化都保存到 `/etc/sysctl.d/98-network-custom.conf`

    $ sudo vi /etc/sysctl.d/98-network-custom.conf
    net.core.rmem_max = 67108864
    net.core.wmem_max = 67108864

67108864 bytes = 64 MB，这个值是比较大的，最好是测试一下，对于你的系统，这个值是否是最优值

最大队列大小优化
----------

在通过TCP / UDP层处理数据之前，系统会将数据放入内核队列中。 `net.core.netdev_max_backlog` 值指定在传递到上层之前要放入队列的最大数据包数。 对于高网络负载，默认值是不够的，因此简单地增加此值可以解决内核导致的性能损失问题。 要查看默认值，请将sysctl与$ sysctl net.core.netdev_max_backlog一起使用。 默认值为1000，将其增加到3000以上将足以阻止数据包在10Gbps（或更多）网络中被丢弃

    $ sysctl net.core.netdev_max_backlog
    sysctl net.core.netdev_max_backlog = 1000

    sudo vi /etc/sysctl.d/98-network-custom.conf
    net.core.netdev_max_backlog = 4096

另一个类似的设置是 `net.ipv4.tcp_max_syn_backlog` 记住的连接请求的最大数量，但仍未收到来自连接客户端的确认。 对于内存超过128 MB的系统，默认值为1024，对于低内存计算机，默认值为128。 如果服务器过载，请尝试增加此数量

    $ sysctl net.ipv4.tcp_max_syn_backlog
    net.ipv4.tcp_max_syn_backlog = 128

    sudo vi /etc/sysctl.d/98-network-custom.conf
    net.ipv4.tcp_max_syn_backlog = 4096

最大挂起连接数优化
----------

应用程序可以在处理一个连接之前指定要放入队列的最大待处理请求数。 当此值达到最大值时，进一步的连接开始退出。 对于发布大量连接的Web服务器等应用程序，此值必须很高才能使这些连接正常工作

    $ sysctl net.core.somaxconn
    net.core.somaxconn = 128

    $ sudo vi /etc/sysctl.d/98-network-custom.conf
    net.core.somaxconn = 4096

TCP缓冲区大小优化
------------

    $ sysctl net.ipv4.tcp_rmem
    net.ipv4.tcp_rmem = 4096        87380   6291456

这些值是三个整数的数组，分别指定TCP读取和发送缓冲区的最小，平均和最大值

注意：值以页为单位。 要查看页面大小，请使用命令查看：

    $ getconf PAGE_SIZE
    4098

也就是设置的值必须是4098的倍数：

    $ sudo vi /etc/sysctl.d/98-network-custom.conf
    net.ipv4.tcp_rmem = 4096 87380 67108864
    net.ipv4.tcp_wmem = 4096 65536 67108864

TCP FIN超时优化
------------

在TCP连接中，双方必须独立关闭连接。 Linux TCP发送FIN数据包以关闭连接并等待FINACK直到定义超时值

    $ sysctl net.ipv4.tcp_fin_timeout
    sysctl net.ipv4.tcp_fin_timeout = 60

    $ sudo vi /etc/sysctl.d/98-network-custom.conf
    net.ipv4.tcp_fin_timeout = 30

默认值（60）非常高，可以减少到20或30以使TCP关闭连接并释放资源以进行另一个连接

IP端口范围优化
-------------

net.ipv4.ip_local_port_range 显示可用于新连接的所有端口。 如果没有空闲端口，则连接将被取消。 增加此值有助于防止此问题

    $ sysctl net.ipv4.ip_local_port_range
    net.ipv4.ip_local_port_range = 32768    60999

    $ sudo vi /etc/sysctl.d/98-network-custom.conf
    net.ipv4.ip_local_port_range = 10000 65535

重用 TIME_WAIT 状态的套接字进行新连接
----------------------

根据[Linux文档](http://lxr.linux.no/#linux+v3.2.8/Documentation/networking/ip-sysctl.txt#L464)，您应该使用TCP_TW_REUSE 标志允许重新使用TIME_WAIT状态的套接字进行新连接

在处理必须处理TIME_WAIT状态下的许多短TCP连接的Web服务器时，这似乎是一个不错的选择

    $ sysctl net.ipv4.tcp_tw_reuse
    net.ipv4.tcp_tw_reuse = 0

    $ sudo vi /etc/sysctl.d/98-network-custom.conf
    net.ipv4.tcp_tw_reuse = 1

Time Wait优化
---------

TIME WAIT TCP套接字状态是套接字关闭但等待处理仍在网络中的数据包的状态。 参数tcp_max_tw_buckets是 TIME_WAIT 状态下的最大套接字数。 达到此数字后，系统将开始在此状态下销毁套接字

此限制仅用于防止简单的DoS攻击，您不得人为地降低限制，而是增加它（可能在增加安装的内存之后），如果网络条件需要超过默认值

    $ sysctl net.ipv4.tcp_max_tw_buckets
    net.ipv4.tcp_max_tw_buckets = 4096

    $ sudo vi /etc/sysctl.d/98-network-custom.conf
    net.ipv4.tcp_max_tw_buckets = 6000

tcp_keepalive_time 优化
----------------

TCP连接由两个套接字组成，每个套接字在连接的两端。 当一方想要终止连接时，它会发送另一方确认的RST数据包并关闭其套接字

然而，在此之前，双方将无限期地保持其套接字开放。 这使得一方可能有意或由于某些错误而关闭其插座，而无需通过RST通知另一端。 为了检测此场景并关闭过时连接，使用TCP Keep Alive处理

有三个可配置属性可确定Keep-Alives的属性。 在Linux上他们是1：

- tcp_keepalive_time

    默认7200秒
- tcp_keepalive_probes

    默认9
- tcp_keepalive_intvl

    默认75秒

这个过程是这样的：

- 客户端打开TCP连接
- 如果tcp_keepalive_time秒的连接是静默的，则发送一个空的ACK数据包
- 服务器是否使用自己的相应ACK进行响应？
  - 没有
    - 等待tcp_keepalive_intvl秒，然后发送另一个ACK
    - 重复，直到已发送的ACK探测数等于tcp_keepalive_probes
    - 如果此时未收到响应，请发送RST并终止连接
  - 是：返回第2步

默认情况下，在大多数操作系统上启用了此处理过程，因此一旦另一端无响应2小时11分钟（7200秒+ 75 * 9秒），则会定期移除死TCP连接

    $ sysctl net.ipv4.tcp_keepalive_time
    net.ipv4.tcp_keepalive_time = 7200

    $ sudo vi /etc/sysctl.d/98-network-custom.conf
    net.ipv4.tcp_keepalive_time = 1200

tcp_mtu_probing 优化
----------

tcp_mtu_probing - INTEGER

控制TCP分组化 - 层路径MTU发现。 可选三个
值：

- 0 已禁用
- 1 默认情况下禁用，在检测到ICMP黑洞时启用
- 2 始终启用，使用tcp_base_mss的初始MSS

运行命令：

    $ sysctl net.ipv4.tcp_mtu_probing
    net.ipv4.tcp_mtu_probing = 0

    $ sudo vi /etc/sysctl.d/98-network-custom.conf
    net.ipv4.tcp_mtu_probing = 1

无需优化
------

- net.ipv4.tcp_syncookies

    Ubuntu 18.04 以下已经默认设置：

      net.ipv4.tcp_syncookies = 1

- net.ipv4.tcp_tw_recycle

    在 Linux 内核 4.12 开始已经移除这个选项了，ubuntu 18.04 不需要设置此值。如果你的内核较旧，可以增加设置：

      net.ipv4.tcp_tw_recycle = 0

[/etc/sysctl.d/98-network-custom.conf](https://github.com/softwaredownload/openwrt-fanqiang/blob/master/ubuntu/etc/sysctl.d/98-network-custom.conf)
-------------

    net.core.rmem_max = 67108864
    net.core.wmem_max = 67108864
    net.core.netdev_max_backlog = 4096
    net.ipv4.tcp_max_syn_backlog = 4096
    net.core.somaxconn = 4096

    net.ipv4.tcp_rmem = 4096 87380 67108864
    net.ipv4.tcp_wmem = 4096 65536 67108864

    net.ipv4.tcp_fin_timeout = 30

    net.ipv4.ip_local_port_range = 10000 65535

    net.ipv4.tcp_tw_reuse = 1
    net.ipv4.tcp_max_tw_buckets = 6000

    net.ipv4.tcp_keepalive_time = 1200

    net.ipv4.tcp_mtu_probing = 1

使用 Linux 系统网络优化立即生效：

    sudo sysctl --system

在Ubuntu 18.04 系统上，我们经过逐项对照系统的默认值，得到上面的优化设置。如果你的系统不是 Ubuntu 18.04，可以在 [Digital Ocean](https://m.do.co/c/89497bd485e0) 创建一个新的 VPS，默认就是最新的 Ubuntu 系统，于就是可以按照本 [科学上网教程](https://github.com/softwaredownload/openwrt-fanqiang) 的方案进行系统优化

**相关资源**:

- https://github.com/softwaredownload/openwrt-fanqiang/tree/master/ubuntu/etc/sysctl.d
- https://github.com/shadowsocks/shadowsocks/wiki/Optimizing-Shadowsocks
- https://opensourceforu.com/2016/10/network-performance-monitoring/